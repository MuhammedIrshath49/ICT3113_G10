{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb8e4733",
   "metadata": {
    "id": "bb8e4733"
   },
   "source": [
    "# Agent CFO â€” Performance Optimization & Design\n",
    "\n",
    "---\n",
    "This is the starter notebook for your project. Follow the required structure below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wkMIj4Ssetku",
   "metadata": {
    "id": "wkMIj4Ssetku"
   },
   "source": [
    "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
    "\n",
    "Your system must:\n",
    "*   Ingest the companyâ€™s public filings.\n",
    "*   Retrieve relevant passages efficiently.\n",
    "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
    "*   Produce answers with valid citations to the correct page/table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c138dd7",
   "metadata": {
    "id": "0c138dd7"
   },
   "source": [
    "## 1. Config & Secrets\n",
    "\n",
    "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a6098a4",
   "metadata": {
    "id": "8a6098a4",
    "ExecuteTime": {
     "end_time": "2025-09-30T06:26:27.439401900Z",
     "start_time": "2025-09-30T06:26:27.414920300Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Example:\n",
    "# os.environ['GEMINI_API_KEY'] = 'your-key-here'\n",
    "# os.environ['OPENAI_API_KEY'] = 'your-key-here'\n",
    "\n",
    "COMPANY_NAME = \"DBS Bank\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7a81e9",
   "metadata": {
    "id": "8b7a81e9"
   },
   "source": [
    "## 2. Data Download (Dropbox)\n",
    "\n",
    "*   Annual Reports: last 3â€“5 years.\n",
    "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
    "*   Investor Presentations and Press Releases.\n",
    "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
    "*   Upload them under `/content/data/`.\n",
    "\n",
    "Scope limit: each team will ingest minimally 15 PDF files total.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4e754",
   "metadata": {
    "id": "b0d4e754"
   },
   "source": [
    "## 3. System Requirements\n",
    "\n",
    "**Retrieval & RAG**\n",
    "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
    "*   Citations must include: report name, year, page number, section/table.\n",
    "\n",
    "**Agentic Reasoning**\n",
    "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
    "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
    "\n",
    "**Instrumentation**\n",
    "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
    "*   Log: tokens used, cache hits, tools invoked.\n",
    "*   Record p50/p95 latencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e08f5a0",
   "metadata": {
    "id": "5e08f5a0",
    "ExecuteTime": {
     "end_time": "2025-09-30T06:27:41.522327700Z",
     "start_time": "2025-09-30T06:26:27.439401900Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "224a4512f22440b6a752809caa6f8d01"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7970b9d207a54f96992c55249140f12a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c2d13178cdd43a2a8f2fda5daecea5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "956f5bc53e8544e491a68ee1dee6bb8b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3377ba759d5340e29288818be3ec2d7e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a18725302e2442ea0bfb199225616bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "773a3d4c7618447d9fad1c5ae1705eeb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "70b4ab8f4af84593bb7fb82272b3b658"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2313ec50bb4a469f990535dd74a53233"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b329079b5bd4795a3c746cfd6a54840"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa2c4b3134c24cb7bf015dfe332a590c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Initialized CFO RAG Pipeline\n",
      "INFO:__main__:Extracted 30 text chunks from content\\data\\2Q22_performance_summary.pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Starting document ingestion ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extracted 31 text chunks from content\\data\\2Q23_performance_summary.pdf\n",
      "INFO:__main__:Extracted 33 text chunks from content\\data\\2Q24_performance_summary.pdf\n",
      "INFO:__main__:Extracted 34 text chunks from content\\data\\2Q25_performance_summary.pdf\n",
      "INFO:__main__:Extracted 44 text chunks from content\\data\\4Q22_performance_summary.pdf\n",
      "INFO:__main__:Extracted 43 text chunks from content\\data\\4Q23_performance_summary.pdf\n",
      "INFO:__main__:Extracted 45 text chunks from content\\data\\4Q24_performance_summary.pdf\n",
      "INFO:__main__:Extracted 115 text chunks from content\\data\\DBS Annual Report 2023.pdf\n",
      "INFO:__main__:Extracted 114 text chunks from content\\data\\dbs-annual-report-2022.pdf\n",
      "INFO:__main__:Extracted 140 text chunks from content\\data\\dbs-annual-report-2024.pdf\n",
      "INFO:__main__:Extracted 19 text chunks from content\\data\\QuartelyResults_1Q23_CFO.pdf\n",
      "INFO:__main__:Extracted 17 text chunks from content\\data\\QuartelyResults_1Q24_CFO.pdf\n",
      "INFO:__main__:Extracted 18 text chunks from content\\data\\QuartelyResults_1Q25_CFO.pdf\n",
      "INFO:__main__:Extracted 30 text chunks from content\\data\\QuartelyResults_2Q23_CFO.pdf\n",
      "INFO:__main__:Extracted 30 text chunks from content\\data\\QuartelyResults_2Q24_CFO.pdf\n",
      "INFO:__main__:Extracted 29 text chunks from content\\data\\QuartelyResults_2Q25_CFO.pdf\n",
      "INFO:__main__:Extracted 18 text chunks from content\\data\\QuartelyResults_3Q23_CFO.pdf\n",
      "INFO:__main__:Extracted 21 text chunks from content\\data\\QuartelyResults_3Q24_CFO.pdf\n",
      "INFO:__main__:Extracted 31 text chunks from content\\data\\QuartelyResults_4Q23_CFO.pdf\n",
      "INFO:__main__:Extracted 30 text chunks from content\\data\\QuartelyResults_4Q24_CFO.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/28 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c43e985ba2024f498e90781e03972020"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saved ingestion data to disk\n",
      "INFO:__main__:Ingested 872 documents in 25.78 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: 20 PDFs\n",
      "Created: 872 text chunks\n",
      "Ingestion Time: 25.78 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df5ddbd25b6244269b9f0d2c90f615ab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Retrieval Test ===\n",
      "Query: Net Interest Margin trend over the past 3 years\n",
      "Retrieved 3 documents:\n",
      "\n",
      "Document 1: DBS Annual Report 2023, Page 13\n",
      "Combined Score: 11.3025\n",
      "Text Preview: 20 DBS ANNUAL REPORT 2023       BUILDING A SUSTAINABLE ADVANTAGE CFO statement We achieved a record performance for the third consecutive year with  t...\n",
      "\n",
      "Document 2: 2Q22_performance_summary, Page 7\n",
      "Combined Score: 11.0513\n",
      "Text Preview: DBS GROUP HOLDINGS LTD AND ITS SUBSIDIARIES    5    First Half    First-half net profit was $3.62 billion, 3% below the  previous yearâ€™s record. Busin...\n",
      "\n",
      "Document 3: QuartelyResults_1Q25_CFO, Page 5\n",
      "Combined Score: 5.1649\n",
      "Text Preview: 5 Net interest margin (%) 2.14 2.14 2.11 2.15 2.12 2.77 2.83 2.83 2.77 2.68 3,647 3,769 3,796 3,831 3,719 -142 1Q24 -175 2Q24 -199 3Q24 -103 4Q24 -38 ...\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ingestion pipeline\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# RAG related libararies\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import fitz  # PyMuPDF for PDF processing\n",
    "from rank_bm25 import BM25Okapi\n",
    "import google.generativeai as genai # Gemini API for higher token limits\n",
    "\n",
    "# Initialise logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# Tool for financial calculations\n",
    "class CalculatorTool:\n",
    "    def calculate_ratio(self, numerator: float, denominator: float, ratio_name: str = \"\") -> Dict[str, Any]:\n",
    "        try:\n",
    "            if denominator == 0:\n",
    "                return {\"error\": f\"Cannot calculate {ratio_name}: denominator is zero\"}\n",
    "\n",
    "            ratio = (numerator / denominator) * 100 if \"ratio\" in ratio_name.lower() else (numerator / denominator)\n",
    "            return {\n",
    "                \"ratio_name\": ratio_name,\n",
    "                \"numerator\": numerator,\n",
    "                \"denominator\": denominator,\n",
    "                \"result\": round(ratio, 2),\n",
    "                \"formula\": f\"{numerator} / {denominator}\"\n",
    "            }\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "        \n",
    "    def trend_analysis(self, values: List[float], periods: List[str]) -> Dict[str, Any]:\n",
    "        if len(values) != len(periods):\n",
    "            return {\"error\": \"Values and periods must have the same length\"}\n",
    "        \n",
    "        if len(values) < 2:\n",
    "            return {\"error\": \"Need at least two data points for trend analysis\"}\n",
    "        \n",
    "        # Calculate period-over-period changes\n",
    "        changes = []\n",
    "        for i in range(1, len(values)):\n",
    "            if values[i-1] != 0:\n",
    "                pct_change = ((values[i] - values[i-1]) / values[i-1]) * 100\n",
    "                changes.append(round(pct_change, 2))\n",
    "            else:\n",
    "                changes.append(0)\n",
    "\n",
    "        return {\n",
    "            \"periods\": periods,\n",
    "            \"values\": values,\n",
    "            \"period_changes\": changes,\n",
    "            \"overall_trend\": \"increasing\" if values[-1] > values[0] else \"decreasing\",\n",
    "            \"average_change\": round(sum(changes) / len(changes), 2) if changes else 0\n",
    "        }\n",
    "    \n",
    "\n",
    "# Tool for extracting table from dataset\n",
    "class TableExtractionTool:\n",
    "    def extract_financial_numbers(self, text: str) -> List[Dict[str, Any]]:\n",
    "        # Pattern for numbers with currency/percentage\n",
    "        patterns = [\n",
    "            r'(\\$|S\\$|USD|SGD)?\\s*(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)\\s*(million|billion|thousand|m|bn|k)?',\n",
    "            r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)\\s*(%|percent|basis points|bps)',\n",
    "            r'(NIM|CTI|ROE|ROA|CET1)\\s*[:=]?\\s*(\\d+(?:\\.\\d+)?)\\s*(%|bps)?'\n",
    "        ]\n",
    "\n",
    "        extracted = []\n",
    "        for pattern in patterns:\n",
    "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                extracted.append({\n",
    "                    \"text\": match.group(0),\n",
    "                    \"value\": match.group(2) if len(match.groups()) > 1 else match.group(1),\n",
    "                    \"context\": text[max(0, match.start()-50):match.end()+50]  # 50 chars before and after\n",
    "                })\n",
    "\n",
    "        return extracted\n",
    "    \n",
    "    def parse_table_structure(self, text: str) -> Dict[str, Any]:\n",
    "        lines = text.split('\\n')\n",
    "        table_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            # Look for lines that might be table rows (have multiple numbers/columns)\n",
    "            if re.search(r'\\d.*\\d', line) and ('|' in line or '\\t' in line or len(re.findall(r'\\d+', line)) > 1):\n",
    "                table_lines.append(line.strip())\n",
    "\n",
    "        return {\n",
    "            \"potential_table_rows\": table_lines[:10], # Return first 10 rows\n",
    "            \"row_count\": len(table_lines)\n",
    "        }\n",
    "    \n",
    "\n",
    "# Tool for comparing info across docs\n",
    "class DocumentComparisonTool:\n",
    "    def compare_metrics_across_docs(self, documents: List[Dict], metric_name: str) -> Dict[str, Any]:\n",
    "        comparisons = []\n",
    "        for doc in documents:\n",
    "            # Extract metric from document text\n",
    "            numbers = re.findall(r'\\d+(?:\\.\\d+)?', doc.get('text', ''))\n",
    "            filename = doc.get('metadata', {}).get('filename', 'unknown')\n",
    "            \n",
    "            comparisons.append({\n",
    "                \"document\": filename,\n",
    "                \"metric_candidates\": numbers[:5], # Return first 5 found numbers\n",
    "                \"text_snippet\": doc.get('text', '')[:200] # First 200 chars\n",
    "            })\n",
    "\n",
    "        return {\n",
    "            \"metric_name\": metric_name,\n",
    "            \"comparisons\": comparisons\n",
    "        }\n",
    "        \n",
    "# RAG functions\n",
    "class CFORAGPipeline:\n",
    "    def __init__(self, persist_dir=\"./cfo_rag_data\"):\n",
    "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "        self.persist_dir = persist_dir\n",
    "        self.documents = []\n",
    "        self.document_metadata = []\n",
    "        self.index = None\n",
    "        self.bm25 = None\n",
    "\n",
    "        # Initialise tools\n",
    "        self.calculator_tool = CalculatorTool()\n",
    "        self.table_extraction_tool = TableExtractionTool()\n",
    "        self.doc_comparison_tool = DocumentComparisonTool()\n",
    "\n",
    "        # Create directory for persistence\n",
    "        os.makedirs(self.persist_dir, exist_ok=True)\n",
    "\n",
    "        # Performance tracking\n",
    "        self.metrics = {\n",
    "            'T_ingest': 0,\n",
    "            'T_retrieve': 0,\n",
    "            'T_rerank': 0,        \n",
    "            'documents_ingested': 0,\n",
    "            }\n",
    "        \n",
    "        logger.info(\"Initialized CFO RAG Pipeline\")\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
    "        # for document chunking\n",
    "        chunks = []\n",
    "\n",
    "        try:\n",
    "            doc = fitz.open(pdf_path)\n",
    "            filename = Path(pdf_path).stem\n",
    "\n",
    "            for page_num in range(len(doc)):\n",
    "                page = doc[page_num]\n",
    "                text = page.get_text()\n",
    "\n",
    "                if text.strip():\n",
    "                    # split by sentences/paragraphs for chunking\n",
    "                    paragraphs = text.split('\\n\\n')\n",
    "\n",
    "                    for i, paragraph in enumerate(paragraphs):\n",
    "                        if len(paragraph.strip()) > 50:\n",
    "                            chunk = {\n",
    "                                'text': paragraph.strip(),\n",
    "                                'metadata': {\n",
    "                                    'filename': filename,\n",
    "                                    'page': page_num + 1,\n",
    "                                    'chunk_id': f\"{filename}_p{page_num+1}_c{i+1}\",\n",
    "                                    'source_type': self._classify_document_type(filename)\n",
    "                                }\n",
    "                            }\n",
    "                            chunks.append(chunk)\n",
    "\n",
    "            doc.close()\n",
    "            logger.info(f\"Extracted {len(chunks)} text chunks from {pdf_path}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
    "\n",
    "        return chunks\n",
    "    \n",
    "    def _classify_document_type(self, filename: str) -> str:\n",
    "        # based on filename\n",
    "        filename_lower = filename.lower()\n",
    "        if 'annual' in filename_lower:\n",
    "            return 'annual_report'\n",
    "        elif any(q in filename_lower for q in ['1q', '2q', '3q', '4q', 'quarter']):\n",
    "            return 'quarterly_report'\n",
    "        elif 'performance' in filename_lower:\n",
    "            return 'performance_summary'\n",
    "        else:\n",
    "            return 'financial_report'\n",
    "        \n",
    "        # document ingestion from data directory containing PDFs/datasets\n",
    "    def ingest_documents(self, data_dir: str = \"./content/data\") -> Dict[str, Any]:\n",
    "        # record time taken to ingest the documents\n",
    "        start_time = time.time()\n",
    "\n",
    "        pdf_files = list(Path(data_dir).glob(\"*.pdf\"))\n",
    "        if not pdf_files:\n",
    "            raise ValueError(f\"No PDF files found in {data_dir}\")\n",
    "        \n",
    "        all_chunks = []\n",
    "\n",
    "        # process each PDF file\n",
    "        for pdf_file in pdf_files:\n",
    "            chunks = self.extract_text_from_pdf(str(pdf_file))\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        # separate text and metadata\n",
    "        texts = [chunk['text'] for chunk in all_chunks]\n",
    "        metadatas = [chunk['metadata'] for chunk in all_chunks]\n",
    "\n",
    "        self.documents = texts\n",
    "        self.document_metadata = metadatas\n",
    "\n",
    "        # Create embeddings\n",
    "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
    "\n",
    "        # Create FAISS index\n",
    "        dimension = embeddings.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dimension)\n",
    "        self.index.add(embeddings.astype('float32'))\n",
    "\n",
    "        # create BM25 index for keyword search\n",
    "        tokenised_docs = [doc.lower().split() for doc in texts]\n",
    "        self.bm25 = BM25Okapi(tokenised_docs)\n",
    "\n",
    "        # save data\n",
    "        self._save_data()\n",
    "\n",
    "        # update metrics\n",
    "        self.metrics['T_ingest'] = time.time() - start_time\n",
    "        self.metrics['documents_ingested'] = len(texts)\n",
    "        logger.info(f\"Ingested {len(texts)} documents in {self.metrics['T_ingest']:.2f} seconds\")\n",
    "\n",
    "        return {\n",
    "            'documents_processed': len(pdf_files),\n",
    "            'chunks_created': len(texts),\n",
    "            'ingestion_duration': self.metrics['T_ingest']\n",
    "        }\n",
    "    \n",
    "    # retrieve relevant documents using hybrid search\n",
    "    def hybrid_retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
    "        start_time = time.time()\n",
    "\n",
    "        if not self.documents or self.index is None:\n",
    "            return []\n",
    "        \n",
    "        try: \n",
    "            # Vector search\n",
    "            query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
    "            vector_k = min(top_k * 2, len(self.documents))\n",
    "            distances, indices = self.index.search(query_embedding.astype('float32'), vector_k)\n",
    "\n",
    "            # BM25 keyword search\n",
    "            bm25_scores = self.bm25.get_scores(query.lower().split())\n",
    "\n",
    "            # merge and rerank\n",
    "            combined_results = []\n",
    "            for i, idx in enumerate(indices[0]):\n",
    "                if idx < len(self.documents):\n",
    "                    # normalise scores and combine\n",
    "                    vector_score = 1 / (1 + distances[0][i])  # convert distance to similarity\n",
    "                    bm25_score = bm25_scores[idx] if idx < len(bm25_scores) else 0\n",
    "\n",
    "                    combined_score = vector_score + bm25_score\n",
    "\n",
    "                    result = {\n",
    "                        'text': self.documents[idx],\n",
    "                        'metadata': self.document_metadata[idx],\n",
    "                        'combined_score': combined_score,\n",
    "                        'vector_score': vector_score,\n",
    "                        'bm25_score': bm25_score,\n",
    "                        'citation': f\"{self.document_metadata[idx]['filename']}, Page {self.document_metadata[idx]['page']}\"\n",
    "                    }\n",
    "                    combined_results.append(result)\n",
    "\n",
    "            # sort by combined score and take top k\n",
    "            combined_results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
    "            final_results = combined_results[:top_k]\n",
    "\n",
    "            self.metrics['T_retrieve'] = time.time() - start_time\n",
    "            return final_results\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "        \n",
    "    def _save_data(self):\n",
    "        # Save FAISS index\n",
    "        try:\n",
    "            with open(os.path.join(self.persist_dir, 'documents.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.documents, f)\n",
    "\n",
    "            with open(os.path.join(self.persist_dir, 'metadata.pkl'), 'wb') as f:\n",
    "                pickle.dump(self.document_metadata, f)\n",
    "\n",
    "            if self.index is not None:\n",
    "                faiss.write_index(self.index, os.path.join(self.persist_dir, 'faiss_index.bin'))\n",
    "\n",
    "            if self.bm25 is not None:\n",
    "                with open(os.path.join(self.persist_dir, 'bm25.pkl'), 'wb') as f:\n",
    "                    pickle.dump(self.bm25, f)\n",
    "        \n",
    "            logger.info(\"Saved ingestion data to disk\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving data: {e}\")\n",
    "\n",
    "cfo_rag = CFORAGPipeline()\n",
    "\n",
    "# Ingest documents from data directory\n",
    "print(\"=== Starting document ingestion ===\")\n",
    "ingestion_result = cfo_rag.ingest_documents(data_dir=\"./content/data\")\n",
    "print(f\"Processed: {ingestion_result['documents_processed']} PDFs\")\n",
    "print(f\"Created: {ingestion_result['chunks_created']} text chunks\")\n",
    "print(f\"Ingestion Time: {ingestion_result['ingestion_duration']:.2f} seconds\")\n",
    "\n",
    "# Test retrieval\n",
    "test_query = \"Net Interest Margin trend over the past 3 years\"\n",
    "retrieved_docs = cfo_rag.hybrid_retrieve(test_query, top_k=3)\n",
    "\n",
    "print(f\"\\n=== Retrieval Test ===\")\n",
    "print(f\"Query: {test_query}\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
    "\n",
    "\n",
    "if retrieved_docs:\n",
    "    for i, doc in enumerate(retrieved_docs, 1):\n",
    "        print(f\"\\nDocument {i}: {doc['citation']}\")\n",
    "        print(f\"Combined Score: {doc['combined_score']:.4f}\")\n",
    "        print(f\"Text Preview: {doc['text'][:150].replace(chr(10), ' ')}...\")  # Print first 150 chars\n",
    "else:\n",
    "    print(\"No documents retrieved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffb05fc",
   "metadata": {
    "id": "6ffb05fc"
   },
   "source": [
    "## 4. Baseline Pipeline\n",
    "\n",
    "**Baseline (starting point)**\n",
    "*   Naive chunking.\n",
    "*   Single-pass vector search.\n",
    "*   One LLM call, no caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "540b7020",
   "metadata": {
    "id": "540b7020",
    "ExecuteTime": {
     "end_time": "2025-09-30T08:24:21.830452900Z",
     "start_time": "2025-09-30T08:23:56.922776200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "892c41c000d845909b487b4246047960"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Baseline Answer ===\n",
      "Based on the provided reports, the Net Interest Margin (NIM) trend over the past three years is as follows:\n",
      "\n",
      "*   **2021:** Net Interest Margin continued a declining trend that had been observed since 2019. The Net Interest Margin for the first half of 2022 (1.52%) was five basis points higher than the first half of 2021, suggesting a lower NIM in 2021. (2Q22_performance_summary, Page 7)\n",
      "*   **2022:** Net Interest Margin began to rise, with an increase in the first quarter and an accelerated improvement in the second quarter. For the first half of 2022, NIM was 1.52%. Overall, the quarterly net interest margin increased by 62 basis points over the four quarters of 2022. The full-year net interest margin for 2022 was approximately 1.75% (derived from the 2023 NIM of 2.15% and the 40 basis point expansion in 2023). (DBS Annual Report 2023, Page 13; 2Q22_performance_summary, Page 7)\n",
      "*   **2023:** The full-year Net Interest Margin expanded by 40 basis points to **2.15%**. Quarterly net interest margin continued to rise in the first three quarters of 2023 before easing in the fourth quarter. (DBS Annual Report 2023, Page 13)\n",
      "\n",
      "In summary, Net Interest Margin declined in 2021, began to rise in 2022, and continued its expansion into 2023, reaching 2.15% for the full year.\n",
      "\n",
      "Citations: ['DBS Annual Report 2023, Page 13', '2Q22_performance_summary, Page 7', 'QuartelyResults_1Q25_CFO, Page 5']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement baseline retrieval + generation\n",
    "# =============================\n",
    "# Part 4. Baseline Pipeline\n",
    "# =============================\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"AIzaSyD9nOIYeshnVBJfV9Im7OUJz85CunBka_I\"\n",
    "\n",
    "# Configure Gemini using environment variable\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "\n",
    "# Load model\n",
    "llm_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
    "\n",
    "\n",
    "def baseline_pipeline(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Runs naive RAG pipeline: retrieval + single LLM call.\n",
    "    \"\"\"\n",
    "    # Retrieve relevant docs\n",
    "    retrieved_docs = cfo_rag.hybrid_retrieve(query, top_k=top_k)\n",
    "    if not retrieved_docs:\n",
    "        return {\"error\": \"No documents retrieved.\"}\n",
    "    \n",
    "    # Build context\n",
    "    context = \"\\n\\n\".join([f\"{doc['citation']}: {doc['text']}\" for doc in retrieved_docs])\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = f\"\"\"\n",
    "You are a financial analyst assistant. \n",
    "Answer the user query based only on the provided reports. \n",
    "Include citations (filename + page).\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Reports:\n",
    "{context}\n",
    "\"\"\"\n",
    "    # Call LLM\n",
    "    response = llm_model.generate_content(prompt)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"citations\": [doc[\"citation\"] for doc in retrieved_docs],\n",
    "        \"raw_docs\": [doc[\"text\"][:300] for doc in retrieved_docs],  # preview only\n",
    "        \"answer\": response.text.strip()\n",
    "    }\n",
    "\n",
    "# ðŸ”¹ Example run\n",
    "result = baseline_pipeline(\"Net Interest Margin trend over the past 3 years\", top_k=3)\n",
    "print(\"=== Baseline Answer ===\")\n",
    "print(result[\"answer\"])\n",
    "print(\"\\nCitations:\", result[\"citations\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e9e3ea",
   "metadata": {
    "id": "01e9e3ea"
   },
   "source": [
    "## 5. Benchmark Runner\n",
    "\n",
    "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
    "\n",
    "*   Net Interest Margin (NIM) trend over last 5 quarters, values and 1â€“2 lines of explanation.\n",
    "    *   Expected: quarterly financial highlights.\n",
    "*   Operating Expenses (Opex) YoY for last 3 years; top 3 drivers from MD&A.\n",
    "    *   Expected: Opex table + MD&A commentary.\n",
    "*   Cost-to-Income Ratio (CTI) for last 3 years; show working + implications.\n",
    "    *   Expected: Operating Income & Opex lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7bddc40",
   "metadata": {
    "id": "e7bddc40",
    "ExecuteTime": {
     "end_time": "2025-09-30T08:27:50.412214800Z",
     "start_time": "2025-09-30T08:25:53.422522300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Benchmark: NIM Trend ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5ee6c59bd3c24603b2bc85a65a93696f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Benchmark: Opex YoY ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4cda989f282b497f90a1a2c8e9ceec5b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Benchmark: Cost-to-Income Ratio ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "257c8e60c7a642d986968898da58b418"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== NIM Trend ===\n",
      "Query: Net Interest Margin (NIM) trend over last 5 quarters, values and 1â€“2 lines of explanation.\n",
      "Answer: The Net Interest Margin (NIM) trend for the last 5 quarters is as follows:\n",
      "\n",
      "*   **1Q24:** 2.14% (QuartelyResults_1Q25_CFO, Page 5)\n",
      "*   **2Q24:** 2.11% (QuartelyResults_1Q25_CFO, Page 5)\n",
      "*   **3Q24:** 2.15% (QuartelyResults_1Q25_CFO, Page 5)\n",
      "*   **4Q24:** 2.12% (QuartelyResults_1Q25_CFO, Page 5)\n",
      "*   **1Q25:** 2.14% (QuartelyResults_1Q25_CFO, Page 5)\n",
      "\n",
      "The Group NIM has generally remained stable over the last five quarters, fluctuating within a narrow range between 2.11% and 2.15%.\n",
      "\n",
      "Citations: ['dbs-annual-report-2022, Page 15', 'QuartelyResults_4Q23_CFO, Page 7', 'QuartelyResults_1Q25_CFO, Page 5', 'QuartelyResults_1Q23_CFO, Page 5', 'QuartelyResults_1Q24_CFO, Page 5']\n",
      "\n",
      "=== Opex YoY ===\n",
      "Query: Operating Expenses (Opex) YoY for last 3 years; top 3 drivers from MD&A.\n",
      "Answer: Based on the provided reports:\n",
      "\n",
      "**Operating Expenses (Opex) YoY for last 3 years:**\n",
      "\n",
      "*   **FY24:** S$8,895 million, a +10% increase YoY (QuartelyResults_4Q24_CFO, Page 3)\n",
      "*   **FY23:** S$8,056 million, a +14% increase YoY (QuartelyResults_4Q23_CFO, Page 23)\n",
      "*   **FY22:** S$7,090 million (QuartelyResults_4Q23_CFO, Page 23)\n",
      "\n",
      "**Top 3 drivers from MD&A:**\n",
      "\n",
      "1.  **Impact from Citi Taiwan:** Citi Taiwan acquisition accounted for 3 percentage points of the 10% expense increase in FY24 (QuartelyResults_4Q24_CFO, Page 3). It was also noted as a factor in expense growth in 4Q23 (QuartelyResults_4Q23_CFO, Page 5).\n",
      "2.  **Higher Staff Costs:** Underlying expenses for 3Q23 were up from higher staff cost (QuartelyResults_3Q23_CFO, Page 4). For FY23, Staff expenses increased by 15% (QuartelyResults_4Q23_CFO, Page 23).\n",
      "3.  **Non-recurring technology and other costs:** These were noted as factors contributing to expense growth in 4Q23 (QuartelyResults_4Q23_CFO, Page 5). For FY23, \"Other expenses\" increased by 11% (QuartelyResults_4Q23_CFO, Page 23).\n",
      "\n",
      "Citations: ['QuartelyResults_4Q24_CFO, Page 3', 'QuartelyResults_1Q25_CFO, Page 3', 'QuartelyResults_4Q23_CFO, Page 5', 'QuartelyResults_4Q23_CFO, Page 23', 'QuartelyResults_3Q23_CFO, Page 4']\n",
      "\n",
      "=== Cost-to-Income Ratio ===\n",
      "Query: Cost-to-Income Ratio (CTI) for last 3 years; show working + implications.\n",
      "Answer: Based on the provided reports, full-year Cost-to-Income Ratios (CTI) are available for the last two fiscal years (FY2023 and FY2022). Full-year CTI data for FY2021 is not available in the provided reports.\n",
      "\n",
      "### Cost-to-Income Ratio (CTI)\n",
      "\n",
      "**1. FY2023**\n",
      "*   **Cost-to-Income Ratio (CTI):** 40% (QuartelyResults_4Q23_CFO, Page 12)\n",
      "*   **Working:**\n",
      "    *   Staff expenses: S$5,036m (QuartelyResults_4Q23_CFO, Page 12)\n",
      "    *   Other expenses: S$3,020m (QuartelyResults_4Q23_CFO, Page 12)\n",
      "    *   Total Expenses = Staff expenses + Other expenses = S$5,036m + S$3,020m = S$8,056m\n",
      "    *   Since CTI = (Total Expenses / Total Income) * 100%, we can infer Total Income:\n",
      "        *   Total Income = Total Expenses / (CTI / 100) = S$8,056m / (40 / 100) = S$8,056m / 0.40 = S$20,140m\n",
      "\n",
      "**2. FY2022**\n",
      "*   **Cost-to-Income Ratio (CTI):** 43% (QuartelyResults_4Q23_CFO, Page 12)\n",
      "*   **Working:**\n",
      "    *   Staff expenses: S$4,376m (QuartelyResults_4Q23_CFO, Page 12)\n",
      "    *   Other expenses: S$2,714m (QuartelyResults_4Q23_CFO, Page 12)\n",
      "    *   Total Expenses = Staff expenses + Other expenses = S$4,376m + S$2,714m = S$7,090m\n",
      "    *   Since CTI = (Total Expenses / Total Income) * 100%, we can infer Total Income:\n",
      "        *   Total Income = Total Expenses / (CTI / 100) = S$7,090m / (43 / 100) = S$7,090m / 0.43 $\\approx$ S$16,488.37m\n",
      "\n",
      "### Implications\n",
      "\n",
      "The Cost-to-Income Ratio (CTI) is a key measure of operational efficiency, indicating how well a company is managing its costs relative to its income. A lower CTI generally signifies better efficiency and profitability.\n",
      "\n",
      "*   **Trend:** The company's CTI has improved (decreased) from 43% in FY2022 to 40% in FY2023.\n",
      "*   **Significance:** This decrease indicates an improvement in operational efficiency. It suggests that the company's income has grown faster than its expenses, or that it has effectively controlled its expenses relative to its income growth. This trend is positive as it implies better cost management and contributes to stronger profitability, reflecting effective strategic and operational execution.\n",
      "\n",
      "Citations: ['QuartelyResults_3Q24_CFO, Page 2', 'QuartelyResults_3Q24_CFO, Page 14', 'QuartelyResults_3Q23_CFO, Page 11', 'QuartelyResults_1Q23_CFO, Page 10', 'QuartelyResults_4Q23_CFO, Page 12']\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement benchmark runner\n",
    "# =============================\n",
    "# Part 5. Benchmark Runner\n",
    "# =============================\n",
    "\n",
    "benchmark_queries = [\n",
    "    {\n",
    "        \"name\": \"NIM Trend\",\n",
    "        \"query\": \"Net Interest Margin (NIM) trend over last 5 quarters, values and 1â€“2 lines of explanation.\",\n",
    "        \"expected\": \"Quarterly financial highlights\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Opex YoY\",\n",
    "        \"query\": \"Operating Expenses (Opex) YoY for last 3 years; top 3 drivers from MD&A.\",\n",
    "        \"expected\": \"Opex table + MD&A commentary\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Cost-to-Income Ratio\",\n",
    "        \"query\": \"Cost-to-Income Ratio (CTI) for last 3 years; show working + implications.\",\n",
    "        \"expected\": \"Operating Income & Opex lines\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# =============================\n",
    "# Benchmark Runner with Instrumentation\n",
    "# =============================\n",
    "\n",
    "def run_benchmark_instrumented(queries, top_k=5):\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        print(f\"\\n=== Running Benchmark: {q['name']} ===\")\n",
    "        output = instrumented_pipeline(q[\"query\"], top_k=top_k)\n",
    "        results.append({\n",
    "            \"name\": q[\"name\"],\n",
    "            \"query\": q[\"query\"],\n",
    "            \"expected\": q[\"expected\"],\n",
    "            \"citations\": output.get(\"citations\", []),\n",
    "            \"answer\": output.get(\"answer\", \"Error: no answer\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ðŸ”¹ Run the benchmarks\n",
    "benchmark_results = run_benchmark_instrumented(benchmark_queries, top_k=5)\n",
    "\n",
    "# Show answers\n",
    "for res in benchmark_results:\n",
    "    print(f\"\\n=== {res['name']} ===\")\n",
    "    print(f\"Query: {res['query']}\")\n",
    "    print(f\"Answer: {res['answer']}\\n\")\n",
    "    print(f\"Citations: {res['citations']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ebeda",
   "metadata": {
    "id": "683ebeda"
   },
   "source": [
    "## 6. Instrumentation\n",
    "\n",
    "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5425de5",
   "metadata": {
    "id": "d5425de5",
    "ExecuteTime": {
     "end_time": "2025-09-30T08:34:22.385600400Z",
     "start_time": "2025-09-30T08:32:59.635717300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Instrumented Benchmark: NIM Trend ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad84d5b6af474de281b8980ae11d5a9a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Instrumented Benchmark: Opex YoY ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kerwin Loh\\AppData\\Local\\Temp\\ipykernel_14456\\1920559068.py:66: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  logs = pd.concat([logs, pd.DataFrame([new_row])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cfac259f53754444a48b973a552d4e9c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Instrumented Benchmark: Cost-to-Income Ratio ===\n"
     ]
    },
    {
     "data": {
      "text/plain": "Batches:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4d007f40b6fb4623a830dfb72c66c006"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "                                               Query   T_ingest  T_retrieve  \\\n0  Net Interest Margin (NIM) trend over last 5 qu...  25.776473    0.052836   \n1  Operating Expenses (Opex) YoY for last 3 years...  25.776473    0.037713   \n2  Cost-to-Income Ratio (CTI) for last 3 years; s...  25.776473    0.033950   \n\n  T_rerank T_reason  T_generate    T_total Tokens CacheHits             Tools  \n0        0        0   12.336731  12.389567   None         0  [Retriever, LLM]  \n1        0        0   30.162275  30.199989   None         0  [Retriever, LLM]  \n2        0        0   40.106169  40.140119   None         0  [Retriever, LLM]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Query</th>\n      <th>T_ingest</th>\n      <th>T_retrieve</th>\n      <th>T_rerank</th>\n      <th>T_reason</th>\n      <th>T_generate</th>\n      <th>T_total</th>\n      <th>Tokens</th>\n      <th>CacheHits</th>\n      <th>Tools</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Net Interest Margin (NIM) trend over last 5 qu...</td>\n      <td>25.776473</td>\n      <td>0.052836</td>\n      <td>0</td>\n      <td>0</td>\n      <td>12.336731</td>\n      <td>12.389567</td>\n      <td>None</td>\n      <td>0</td>\n      <td>[Retriever, LLM]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Operating Expenses (Opex) YoY for last 3 years...</td>\n      <td>25.776473</td>\n      <td>0.037713</td>\n      <td>0</td>\n      <td>0</td>\n      <td>30.162275</td>\n      <td>30.199989</td>\n      <td>None</td>\n      <td>0</td>\n      <td>[Retriever, LLM]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Cost-to-Income Ratio (CTI) for last 3 years; s...</td>\n      <td>25.776473</td>\n      <td>0.033950</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40.106169</td>\n      <td>40.140119</td>\n      <td>None</td>\n      <td>0</td>\n      <td>[Retriever, LLM]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =============================\n",
    "# Part 6. Instrumentation\n",
    "# =============================\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Create logs DataFrame\n",
    "logs = pd.DataFrame(columns=[\n",
    "    'Query', \n",
    "    'T_ingest', 'T_retrieve', 'T_rerank', 'T_reason', \n",
    "    'T_generate', 'T_total', \n",
    "    'Tokens', 'CacheHits', 'Tools'\n",
    "])\n",
    "\n",
    "def instrumented_pipeline(query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Runs pipeline with instrumentation (timing + usage logging).\n",
    "    \"\"\"\n",
    "    global logs\n",
    "    timings = {}\n",
    "    start_total = time.time()\n",
    "\n",
    "    # --- Retrieval ---\n",
    "    start_retrieve = time.time()\n",
    "    retrieved_docs = cfo_rag.hybrid_retrieve(query, top_k=top_k)\n",
    "    timings['T_retrieve'] = time.time() - start_retrieve\n",
    "\n",
    "    if not retrieved_docs:\n",
    "        return {\"error\": \"No documents retrieved.\"}\n",
    "\n",
    "    # --- Build context ---\n",
    "    context = \"\\n\\n\".join([f\"{doc['citation']}: {doc['text']}\" for doc in retrieved_docs])\n",
    "\n",
    "    # --- Generation ---\n",
    "    start_generate = time.time()\n",
    "    prompt = f\"\"\"\n",
    "    You are a financial analyst assistant. \n",
    "    Answer the query based only on the provided reports.\n",
    "    Always include citations (filename + page).\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Reports:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    response = llm_model.generate_content(prompt)\n",
    "    timings['T_generate'] = time.time() - start_generate\n",
    "\n",
    "    # --- Total time ---\n",
    "    timings['T_total'] = time.time() - start_total\n",
    "\n",
    "    # --- Collect metrics ---\n",
    "    new_row = {\n",
    "        'Query': query,\n",
    "        'T_ingest': cfo_rag.metrics.get('T_ingest', 0),\n",
    "        'T_retrieve': timings['T_retrieve'],\n",
    "        'T_rerank': cfo_rag.metrics.get('T_rerank', 0),\n",
    "        'T_reason': 0,   # placeholder\n",
    "        'T_generate': timings['T_generate'],\n",
    "        'T_total': timings['T_total'],\n",
    "        'Tokens': None,      # Gemini doesnâ€™t return token counts yet\n",
    "        'CacheHits': 0,      # extend later if caching is added\n",
    "        'Tools': ['Retriever', 'LLM']\n",
    "    }\n",
    "    logs = pd.concat([logs, pd.DataFrame([new_row])], ignore_index=True)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"citations\": [doc[\"citation\"] for doc in retrieved_docs],\n",
    "        \"answer\": response.text.strip()\n",
    "    }\n",
    "\n",
    "def run_benchmark_instrumented(queries, top_k=5):\n",
    "    results = []\n",
    "    for q in queries:\n",
    "        print(f\"\\n=== Running Instrumented Benchmark: {q['name']} ===\")\n",
    "        output = instrumented_pipeline(q[\"query\"], top_k=top_k)\n",
    "        results.append({\n",
    "            \"name\": q[\"name\"],\n",
    "            \"query\": q[\"query\"],\n",
    "            \"expected\": q[\"expected\"],\n",
    "            \"citations\": output.get(\"citations\", []),\n",
    "            \"answer\": output.get(\"answer\", \"Error: no answer\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# ðŸ”¹ Run instrumented benchmarks\n",
    "benchmark_results_instrumented = run_benchmark_instrumented(benchmark_queries, top_k=5)\n",
    "\n",
    "# Show instrumentation logs (should have 3 rows now)\n",
    "logs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c01bf4",
   "metadata": {
    "id": "e8c01bf4"
   },
   "source": [
    "## 7. Optimizations\n",
    "\n",
    "**Required Optimizations**\n",
    "\n",
    "Each team must implement at least:\n",
    "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
    "*   1 caching optimization (query cache or ratio cache).\n",
    "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
    "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "783f0e2e",
   "metadata": {
    "id": "783f0e2e",
    "ExecuteTime": {
     "end_time": "2025-09-30T06:27:41.586515200Z",
     "start_time": "2025-09-30T06:27:41.512318200Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Implement optimizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91ce833",
   "metadata": {
    "id": "a91ce833"
   },
   "source": [
    "## 8. Results & Plots\n",
    "\n",
    "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d96550f3",
   "metadata": {
    "id": "d96550f3",
    "ExecuteTime": {
     "end_time": "2025-09-30T06:27:41.586515200Z",
     "start_time": "2025-09-30T06:27:41.517370Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Generate plots with matplotlib\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s2am56tlpjY9",
   "metadata": {
    "id": "s2am56tlpjY9"
   },
   "source": [
    "testing"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
