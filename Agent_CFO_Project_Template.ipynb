{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammedIrshath49/ICT3113_G10/blob/main/Agent_CFO_Project_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb8e4733",
      "metadata": {
        "id": "bb8e4733"
      },
      "source": [
        "# Agent CFO â€” Performance Optimization & Design\n",
        "\n",
        "---\n",
        "This is the starter notebook for your project. Follow the required structure below.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6f8a4ed",
      "metadata": {
        "id": "a6f8a4ed"
      },
      "source": [
        "Colab integrates well with GitHub, allowing you to clone repositories, save notebooks, and share your work.\n",
        "\n",
        "**1. Cloning a Repository**\n",
        "\n",
        "You can clone a public or private GitHub repository directly into your Colab environment using the `!git clone` command. For private repositories, you'll need to provide credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dbeed6e6",
      "metadata": {
        "id": "dbeed6e6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "Updating files:  41% (1620/3943)\n",
            "Updating files:  42% (1657/3943)\n",
            "Updating files:  43% (1696/3943)\n",
            "Updating files:  44% (1735/3943)\n",
            "Updating files:  45% (1775/3943)\n",
            "Updating files:  46% (1814/3943)\n",
            "Updating files:  47% (1854/3943)\n",
            "Updating files:  48% (1893/3943)\n",
            "Updating files:  49% (1933/3943)\n",
            "Updating files:  50% (1972/3943)\n",
            "Updating files:  51% (2011/3943)\n",
            "Updating files:  52% (2051/3943)\n",
            "Updating files:  53% (2090/3943)\n",
            "Updating files:  54% (2130/3943)\n",
            "Updating files:  55% (2169/3943)\n",
            "Updating files:  56% (2209/3943)\n",
            "Updating files:  57% (2248/3943)\n",
            "Updating files:  58% (2287/3943)\n",
            "Updating files:  59% (2327/3943)\n",
            "Updating files:  60% (2366/3943)\n",
            "Updating files:  61% (2406/3943)\n",
            "Updating files:  62% (2445/3943)\n",
            "Updating files:  63% (2485/3943)\n",
            "Updating files:  64% (2524/3943)\n",
            "Updating files:  65% (2563/3943)\n",
            "Updating files:  66% (2603/3943)\n",
            "Updating files:  67% (2642/3943)\n",
            "Updating files:  68% (2682/3943)\n",
            "Updating files:  69% (2721/3943)\n",
            "Updating files:  70% (2761/3943)\n",
            "Updating files:  71% (2800/3943)\n",
            "Updating files:  72% (2839/3943)\n",
            "Updating files:  73% (2879/3943)\n",
            "Updating files:  74% (2918/3943)\n",
            "Updating files:  75% (2958/3943)\n",
            "Updating files:  76% (2997/3943)\n",
            "Updating files:  76% (3023/3943)\n",
            "Updating files:  77% (3037/3943)\n",
            "Updating files:  78% (3076/3943)\n",
            "Updating files:  79% (3115/3943)\n",
            "Updating files:  80% (3155/3943)\n",
            "Updating files:  81% (3194/3943)\n",
            "Updating files:  82% (3234/3943)\n",
            "Updating files:  83% (3273/3943)\n",
            "Updating files:  84% (3313/3943)\n",
            "Updating files:  85% (3352/3943)\n",
            "Updating files:  86% (3391/3943)\n",
            "Updating files:  87% (3431/3943)\n",
            "Updating files:  88% (3470/3943)\n",
            "Updating files:  89% (3510/3943)\n",
            "Updating files:  90% (3549/3943)\n",
            "Updating files:  91% (3589/3943)\n",
            "Updating files:  92% (3628/3943)\n",
            "Updating files:  93% (3667/3943)\n",
            "Updating files:  94% (3707/3943)\n",
            "Updating files:  95% (3746/3943)\n",
            "Updating files:  96% (3786/3943)\n",
            "Updating files:  97% (3825/3943)\n",
            "Updating files:  98% (3865/3943)\n",
            "Updating files:  99% (3904/3943)\n",
            "Updating files: 100% (3943/3943)\n",
            "Updating files: 100% (3943/3943), done.\n"
          ]
        }
      ],
      "source": [
        "# Clone a public repository\n",
        "!git clone https://github.com/tensorflow/models.git\n",
        "\n",
        "# For a private repository, you might need to use credentials\n",
        "# !git clone https://<YOUR_GITHUB_USERNAME>:<YOUR_GITHUB_TOKEN>@github.com/<USERNAME>/<REPOSITORY>.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d74b9f6",
      "metadata": {
        "id": "5d74b9f6"
      },
      "source": [
        "**2. Saving a Notebook to GitHub**\n",
        "\n",
        "You can save your Colab notebook directly to GitHub. Go to `File > Save a copy to GitHub`. You'll be prompted to authorize Colab to access your GitHub account and then you can select the repository and commit message.\n",
        "\n",
        "**3. Loading a Notebook from GitHub**\n",
        "\n",
        "To open a notebook from GitHub, go to `File > Open notebook`. In the dialog, select the \"GitHub\" tab and enter the GitHub URL or search for the repository and notebook.\n",
        "\n",
        "**4. Authenticating with GitHub**\n",
        "\n",
        "If you need to perform actions like pushing changes to a private repository, you'll need to authenticate. You can use a Personal Access Token (PAT) with the necessary permissions. Store your PAT securely (e.g., in Colab's Secrets Manager) and use it in your commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "11f946f8",
      "metadata": {
        "id": "11f946f8"
      },
      "outputs": [],
      "source": [
        "# Example of configuring git with your name and email\n",
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\"\n",
        "\n",
        "# Example of using a PAT for authentication (replace with your secret name)\n",
        "# from google.colab import userdata\n",
        "# github_token = userdata.get('github_pat')\n",
        "# !git remote set-url origin https://<YOUR_GITHUB_USERNAME>:$github_token@github.com/<USERNAME>/<REPOSITORY>.git"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9eda7863",
      "metadata": {
        "id": "9eda7863"
      },
      "source": [
        "These are the basic steps to get you started with using GitHub in Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wkMIj4Ssetku",
      "metadata": {
        "id": "wkMIj4Ssetku"
      },
      "source": [
        "You will design and optimize an Agent CFO assistant for a listed company. The assistant should answer finance/operations questions using RAG (Retrieval-Augmented Generation) + agentic reasoning, with response time (latency) as the primary metric.\n",
        "\n",
        "Your system must:\n",
        "*   Ingest the companyâ€™s public filings.\n",
        "*   Retrieve relevant passages efficiently.\n",
        "*   Compute ratios/trends via tool calls (calculator, table parsing).\n",
        "*   Produce answers with valid citations to the correct page/table.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c138dd7",
      "metadata": {
        "id": "0c138dd7"
      },
      "source": [
        "## 1. Config & Secrets\n",
        "\n",
        "Fill in your API keys in secrets. **Do not hardcode keys** in cells."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a6098a4",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T08:55:21.268657600Z",
          "start_time": "2025-09-30T08:55:21.203754400Z"
        },
        "id": "8a6098a4"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "COMPANY_NAME = \"DBS Bank\"\n",
        "DATA_DIR = \"/content/drive/MyDrive/ICT3113 Group 10/Data\" # Define data directory here\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ec3fd90",
      "metadata": {
        "id": "0ec3fd90"
      },
      "source": [
        "### API Keys and Secrets Management\n",
        "\n",
        "This project utilizes the Google Gemini API. To use it, you will need a Gemini API key.\n",
        "\n",
        "**How to obtain a Gemini API Key:**\n",
        "1.  Go to Google AI Studio ([https://aistudio.google.com/](https://aistudio.google.com/)).\n",
        "2.  Create or select a project.\n",
        "3.  Generate an API key.\n",
        "\n",
        "**How to store your API Key securely in Colab Secrets:**\n",
        "1.  In the left sidebar of your Colab notebook, click on the \"ðŸ”‘ Secrets\" tab.\n",
        "2.  Click on \"Add new secret\".\n",
        "3.  For the **Name**, enter `GOOGLE_API_KEY`. This is the name the code will use to access the key.\n",
        "4.  For the **Value**, paste your Gemini API key.\n",
        "5.  Make sure the \"Notebook access\" toggle is turned ON for this secret.\n",
        "\n",
        "You can then access the secret in your code using:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b7a81e9",
      "metadata": {
        "id": "8b7a81e9"
      },
      "source": [
        "## 2. Data Download (Dropbox)\n",
        "\n",
        "*   Annual Reports: last 3â€“5 years.\n",
        "*   Quarterly Results Packs & MD&A (Management Discussion & Analysis).\n",
        "*   Investor Presentations and Press Releases.\n",
        "*   These files must be submitted later as a deliverable in the Dropbox data pack.\n",
        "*   Upload them under `/content/data/`.\n",
        "\n",
        "Scope limit: each team will ingest minimally 15 PDF files total.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0d4e754",
      "metadata": {
        "id": "b0d4e754"
      },
      "source": [
        "## 3. System Requirements\n",
        "\n",
        "**Retrieval & RAG**\n",
        "*   Use a vector index (e.g., FAISS, LlamaIndex) + a keyword filter (BM25/ElasticSearch).\n",
        "*   Citations must include: report name, year, page number, section/table.\n",
        "\n",
        "**Agentic Reasoning**\n",
        "*   Support at least 3 tool types: calculator, table extraction, multi-document compare.\n",
        "*   Reasoning must follow a plan-then-act pattern (not a single unstructured call).\n",
        "\n",
        "**Instrumentation**\n",
        "*   Log timings for: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total.\n",
        "*   Log: tokens used, cache hits, tools invoked.\n",
        "*   Record p50/p95 latencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JsE2rc9-3Ex3",
      "metadata": {
        "id": "JsE2rc9-3Ex3"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uGS12oor3Q7o",
      "metadata": {
        "id": "uGS12oor3Q7o"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers faiss-cpu numpy pandas scikit-learn PyMuPDF rank-bm25 google-generativeai tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e08f5a0",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T09:08:36.387313200Z",
          "start_time": "2025-09-30T09:08:08.875497Z"
        },
        "id": "5e08f5a0"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement ingestion pipeline\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import logging\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pathlib import Path\n",
        "\n",
        "# RAG related libararies\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fitz  # PyMuPDF for PDF processing\n",
        "from rank_bm25 import BM25Okapi\n",
        "import google.generativeai as genai # Gemini API for higher token limits\n",
        "\n",
        "# Initialise logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Tool for financial calculations\n",
        "class CalculatorTool:\n",
        "    def calculate_ratio(self, numerator: float, denominator: float, ratio_name: str = \"\") -> Dict[str, Any]:\n",
        "        try:\n",
        "            if denominator == 0:\n",
        "                return {\"error\": f\"Cannot calculate {ratio_name}: denominator is zero\"}\n",
        "\n",
        "            ratio = (numerator / denominator) * 100 if \"ratio\" in ratio_name.lower() else (numerator / denominator)\n",
        "            return {\n",
        "                \"ratio_name\": ratio_name,\n",
        "                \"numerator\": numerator,\n",
        "                \"denominator\": denominator,\n",
        "                \"result\": round(ratio, 2),\n",
        "                \"formula\": f\"{numerator} / {denominator}\"\n",
        "            }\n",
        "        except Exception as e:\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def trend_analysis(self, values: List[float], periods: List[str]) -> Dict[str, Any]:\n",
        "        if len(values) != len(periods):\n",
        "            return {\"error\": \"Values and periods must have the same length\"}\n",
        "\n",
        "        if len(values) < 2:\n",
        "            return {\"error\": \"Need at least two data points for trend analysis\"}\n",
        "\n",
        "        # Calculate period-over-period changes\n",
        "        changes = []\n",
        "        for i in range(1, len(values)):\n",
        "            if values[i-1] != 0:\n",
        "                pct_change = ((values[i] - values[i-1]) / values[i-1]) * 100\n",
        "                changes.append(round(pct_change, 2))\n",
        "            else:\n",
        "                changes.append(0)\n",
        "\n",
        "        return {\n",
        "            \"periods\": periods,\n",
        "            \"values\": values,\n",
        "            \"period_changes\": changes,\n",
        "            \"overall_trend\": \"increasing\" if values[-1] > values[0] else \"decreasing\",\n",
        "            \"average_change\": round(sum(changes) / len(changes), 2) if changes else 0\n",
        "        }\n",
        "\n",
        "\n",
        "# Tool for extracting table from dataset\n",
        "class TableExtractionTool:\n",
        "    def extract_financial_numbers(self, text: str) -> List[Dict[str, Any]]:\n",
        "        # Pattern for numbers with currency/percentage\n",
        "        patterns = [\n",
        "            r'(\\$|S\\$|USD|SGD)?\\s*(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)\\s*(million|billion|thousand|m|bn|k)?',\n",
        "            r'(\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?)\\s*(%|percent|basis points|bps)',\n",
        "            r'(NIM|CTI|ROE|ROA|CET1)\\s*[:=]?\\s*(\\d+(?:\\.\\d+)?)\\s*(%|bps)?'\n",
        "        ]\n",
        "\n",
        "        extracted = []\n",
        "        for pattern in patterns:\n",
        "            matches = re.finditer(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                extracted.append({\n",
        "                    \"text\": match.group(0),\n",
        "                    \"value\": match.group(2) if len(match.groups()) > 1 else match.group(1),\n",
        "                    \"context\": text[max(0, match.start()-50):match.end()+50]  # 50 chars before and after\n",
        "                })\n",
        "\n",
        "        return extracted\n",
        "\n",
        "    def parse_table_structure(self, text: str) -> Dict[str, Any]:\n",
        "        lines = text.split('\\n')\n",
        "        table_lines = []\n",
        "\n",
        "        for line in lines:\n",
        "            # Look for lines that might be table rows (have multiple numbers/columns)\n",
        "            if re.search(r'\\d.*\\d', line) and ('|' in line or '\\t' in line or len(re.findall(r'\\d+', line)) > 1):\n",
        "                table_lines.append(line.strip())\n",
        "\n",
        "        return {\n",
        "            \"potential_table_rows\": table_lines[:10], # Return first 10 rows\n",
        "            \"row_count\": len(table_lines)\n",
        "        }\n",
        "\n",
        "\n",
        "# Tool for comparing info across docs\n",
        "class DocumentComparisonTool:\n",
        "    def compare_metrics_across_docs(self, documents: List[Dict], metric_name: str) -> Dict[str, Any]:\n",
        "        comparisons = []\n",
        "        for doc in documents:\n",
        "            # Extract metric from document text\n",
        "            numbers = re.findall(r'\\d+(?:\\.\\d+)?', doc.get('text', ''))\n",
        "            filename = doc.get('metadata', {}).get('filename', 'unknown')\n",
        "\n",
        "            comparisons.append({\n",
        "                \"document\": filename,\n",
        "                \"metric_candidates\": numbers[:5], # Return first 5 found numbers\n",
        "                \"text_snippet\": doc.get('text', '')[:200] # First 200 chars\n",
        "            })\n",
        "\n",
        "        return {\n",
        "            \"metric_name\": metric_name,\n",
        "            \"comparisons\": comparisons\n",
        "        }\n",
        "\n",
        "# RAG functions\n",
        "class CFORAGPipeline:\n",
        "    def __init__(self, persist_dir=\"./cfo_rag_data\"):\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        self.persist_dir = persist_dir\n",
        "        self.documents = []\n",
        "        self.document_metadata = []\n",
        "        self.index = None\n",
        "        self.bm25 = None\n",
        "\n",
        "        # Initialise tools\n",
        "        self.calculator_tool = CalculatorTool()\n",
        "        self.table_extraction_tool = TableExtractionTool()\n",
        "        self.doc_comparison_tool = DocumentComparisonTool()\n",
        "\n",
        "        # Create directory for persistence\n",
        "        os.makedirs(self.persist_dir, exist_ok=True)\n",
        "\n",
        "        # Performance tracking\n",
        "        self.metrics = {\n",
        "            'T_ingest': 0,\n",
        "            'T_retrieve': 0,\n",
        "            'T_rerank': 0,\n",
        "            'documents_ingested': 0,\n",
        "            }\n",
        "\n",
        "        logger.info(\"Initialized CFO RAG Pipeline\")\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_path: str) -> List[Dict[str, Any]]:\n",
        "        # for document chunking\n",
        "        chunks = []\n",
        "\n",
        "        try:\n",
        "            doc = fitz.open(pdf_path)\n",
        "            filename = Path(pdf_path).stem\n",
        "\n",
        "            for page_num in range(len(doc)):\n",
        "                page = doc[page_num]\n",
        "                text = page.get_text()\n",
        "\n",
        "                if text.strip():\n",
        "                    # split by sentences/paragraphs for chunking\n",
        "                    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "                    for i, paragraph in enumerate(paragraphs):\n",
        "                        if len(paragraph.strip()) > 50:\n",
        "                            chunk = {\n",
        "                                'text': paragraph.strip(),\n",
        "                                'metadata': {\n",
        "                                    'filename': filename,\n",
        "                                    'page': page_num + 1,\n",
        "                                    'chunk_id': f\"{filename}_p{page_num+1}_c{i+1}\",\n",
        "                                    'source_type': self._classify_document_type(filename)\n",
        "                                }\n",
        "                            }\n",
        "                            chunks.append(chunk)\n",
        "\n",
        "            doc.close()\n",
        "            logger.info(f\"Extracted {len(chunks)} text chunks from {pdf_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting text from {pdf_path}: {e}\")\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _classify_document_type(self, filename: str) -> str:\n",
        "        # based on filename\n",
        "        filename_lower = filename.lower()\n",
        "        if 'annual' in filename_lower:\n",
        "            return 'annual_report'\n",
        "        elif any(q in filename_lower for q in ['1q', '2q', '3q', '4q', 'quarter']):\n",
        "            return 'quarterly_report'\n",
        "        elif 'performance' in filename_lower:\n",
        "            return 'performance_summary'\n",
        "        else:\n",
        "            return 'financial_report'\n",
        "\n",
        "        # document ingestion from data directory containing PDFs/datasets\n",
        "    def ingest_documents(self, data_dir: str = \"./content/data\") -> Dict[str, Any]:\n",
        "        # record time taken to ingest the documents\n",
        "        start_time = time.time()\n",
        "\n",
        "        pdf_files = list(Path(data_dir).glob(\"*.pdf\"))\n",
        "        if not pdf_files:\n",
        "            raise ValueError(f\"No PDF files found in {data_dir}\")\n",
        "\n",
        "        all_chunks = []\n",
        "\n",
        "        # process each PDF file\n",
        "        for pdf_file in pdf_files:\n",
        "            chunks = self.extract_text_from_pdf(str(pdf_file))\n",
        "            all_chunks.extend(chunks)\n",
        "\n",
        "        # separate text and metadata\n",
        "        texts = [chunk['text'] for chunk in all_chunks]\n",
        "        metadatas = [chunk['metadata'] for chunk in all_chunks]\n",
        "\n",
        "        self.documents = texts\n",
        "        self.document_metadata = metadatas\n",
        "\n",
        "        # Create embeddings\n",
        "        embeddings = self.model.encode(texts, convert_to_numpy=True, show_progress_bar=True)\n",
        "\n",
        "        # Create FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatL2(dimension)\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "        # create BM25 index for keyword search\n",
        "        tokenised_docs = [doc.lower().split() for doc in texts]\n",
        "        self.bm25 = BM25Okapi(tokenised_docs)\n",
        "\n",
        "        # save data\n",
        "        self._save_data()\n",
        "\n",
        "        # update metrics\n",
        "        self.metrics['T_ingest'] = time.time() - start_time\n",
        "        self.metrics['documents_ingested'] = len(texts)\n",
        "        logger.info(f\"Ingested {len(texts)} documents in {self.metrics['T_ingest']:.2f} seconds\")\n",
        "\n",
        "        return {\n",
        "            'documents_processed': len(pdf_files),\n",
        "            'chunks_created': len(texts),\n",
        "            'ingestion_duration': self.metrics['T_ingest']\n",
        "        }\n",
        "\n",
        "    # retrieve relevant documents using hybrid search\n",
        "    def hybrid_retrieve(self, query: str, top_k: int = 5) -> List[Dict[str, Any]]:\n",
        "        if not self.documents or self.index is None:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # --- Start retrieval timer ---\n",
        "            start_retrieve = time.time()\n",
        "\n",
        "            # Vector search\n",
        "            query_embedding = self.model.encode([query], convert_to_numpy=True)\n",
        "            vector_k = min(top_k * 2, len(self.documents))\n",
        "            distances, indices = self.index.search(query_embedding.astype('float32'), vector_k)\n",
        "\n",
        "            # BM25 keyword search\n",
        "            bm25_scores = self.bm25.get_scores(query.lower().split())\n",
        "\n",
        "            # Stop retrieval timer (only covers FAISS + BM25)\n",
        "            self.metrics['T_retrieve'] = time.time() - start_retrieve\n",
        "\n",
        "            # --- Start rerank timer ---\n",
        "            start_rerank = time.perf_counter()\n",
        "\n",
        "            combined_results = []\n",
        "            for i, idx in enumerate(indices[0]):\n",
        "                if idx < len(self.documents):\n",
        "                    vector_score = 1 / (1 + distances[0][i])\n",
        "                    bm25_score = bm25_scores[idx] if idx < len(bm25_scores) else 0\n",
        "                    combined_score = vector_score + bm25_score\n",
        "\n",
        "                    result = {\n",
        "                        'text': self.documents[idx],\n",
        "                        'metadata': self.document_metadata[idx],\n",
        "                        'combined_score': combined_score,\n",
        "                        'vector_score': vector_score,\n",
        "                        'bm25_score': bm25_score,\n",
        "                        'citation': f\"{self.document_metadata[idx]['filename']}, Page {self.document_metadata[idx]['page']}\"\n",
        "                    }\n",
        "                    combined_results.append(result)\n",
        "\n",
        "            # Sorting and taking top_k\n",
        "            combined_results.sort(key=lambda x: x['combined_score'], reverse=True)\n",
        "            final_results = combined_results[:top_k]\n",
        "\n",
        "            # Stop rerank timer (store in ms)\n",
        "            self.metrics['T_rerank'] = (time.perf_counter() - start_rerank) * 1000\n",
        "\n",
        "            return final_results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during retrieval: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def _save_data(self):\n",
        "        # Save FAISS index\n",
        "        try:\n",
        "            with open(os.path.join(self.persist_dir, 'documents.pkl'), 'wb') as f:\n",
        "                pickle.dump(self.documents, f)\n",
        "\n",
        "            with open(os.path.join(self.persist_dir, 'metadata.pkl'), 'wb') as f:\n",
        "                pickle.dump(self.document_metadata, f)\n",
        "\n",
        "            if self.index is not None:\n",
        "                faiss.write_index(self.index, os.path.join(self.persist_dir, 'faiss_index.bin'))\n",
        "\n",
        "            if self.bm25 is not None:\n",
        "                with open(os.path.join(self.persist_dir, 'bm25.pkl'), 'wb') as f:\n",
        "                    pickle.dump(self.bm25, f)\n",
        "\n",
        "            logger.info(\"Saved ingestion data to disk\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving data: {e}\")\n",
        "\n",
        "cfo_rag = CFORAGPipeline()\n",
        "\n",
        "# Ingest documents from data directory\n",
        "print(\"=== Starting document ingestion ===\")\n",
        "ingestion_result = cfo_rag.ingest_documents(data_dir=DATA_DIR) # Use the variable here\n",
        "print(f\"Processed: {ingestion_result['documents_processed']} PDFs\")\n",
        "print(f\"Created: {ingestion_result['chunks_created']} text chunks\")\n",
        "print(f\"Ingestion Time: {ingestion_result['ingestion_duration']:.2f} seconds\")\n",
        "\n",
        "# Test retrieval\n",
        "test_query = \"Net Interest Margin trend over the past 3 years\"\n",
        "retrieved_docs = cfo_rag.hybrid_retrieve(test_query, top_k=3)\n",
        "\n",
        "print(f\"\\n=== Retrieval Test ===\")\n",
        "print(f\"Query: {test_query}\")\n",
        "print(f\"Retrieved {len(retrieved_docs)} documents:\")\n",
        "\n",
        "\n",
        "if retrieved_docs:\n",
        "    for i, doc in enumerate(retrieved_docs, 1):\n",
        "        print(f\"\\nDocument {i}: {doc['citation']}\")\n",
        "        print(f\"Combined Score: {doc['combined_score']:.4f}\")\n",
        "        print(f\"Text Preview: {doc['text'][:150].replace(chr(10), ' ')}...\")  # Print first 150 chars\n",
        "else:\n",
        "    print(\"No documents retrieved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ffb05fc",
      "metadata": {
        "id": "6ffb05fc"
      },
      "source": [
        "## 4. Baseline Pipeline\n",
        "\n",
        "**Baseline (starting point)**\n",
        "*   Naive chunking.\n",
        "*   Single-pass vector search.\n",
        "*   One LLM call, no caching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "540b7020",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T09:09:29.110581200Z",
          "start_time": "2025-09-30T09:08:44.027346100Z"
        },
        "id": "540b7020"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement baseline retrieval + generation\n",
        "# =============================\n",
        "# Part 4. Baseline Pipeline\n",
        "# =============================\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure Gemini using Colab secrets\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# Load model\n",
        "llm_model = genai.GenerativeModel(\"gemini-2.5-flash\")\n",
        "\n",
        "\n",
        "def baseline_pipeline(query: str, top_k: int = 5):\n",
        "    \"\"\"\n",
        "    Runs naive RAG pipeline: retrieval + single LLM call.\n",
        "    \"\"\"\n",
        "    # Retrieve relevant docs\n",
        "    retrieved_docs = cfo_rag.hybrid_retrieve(query, top_k=top_k)\n",
        "    if not retrieved_docs:\n",
        "        return {\"error\": \"No documents retrieved.\"}\n",
        "\n",
        "    # Build context\n",
        "    context = \"\\n\\n\".join([f\"{doc['citation']}: {doc['text']}\" for doc in retrieved_docs])\n",
        "\n",
        "    # Prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a financial analyst assistant.\n",
        "Answer the user query based only on the provided reports.\n",
        "Include citations (filename + page).\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "Reports:\n",
        "{context}\n",
        "\"\"\"\n",
        "    # Call LLM\n",
        "    response = llm_model.generate_content(prompt)\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"citations\": [doc[\"citation\"] for doc in retrieved_docs],\n",
        "        \"raw_docs\": [doc[\"text\"][:300] for doc in retrieved_docs],  # preview only\n",
        "        \"answer\": response.text.strip()\n",
        "    }\n",
        "\n",
        "# ðŸ”¹ Example run\n",
        "result = baseline_pipeline(\"Net Interest Margin trend over the past 3 years\", top_k=3)\n",
        "print(\"=== Baseline Answer ===\")\n",
        "print(result[\"answer\"])\n",
        "print(\"\\nCitations:\", result[\"citations\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e9e3ea",
      "metadata": {
        "id": "01e9e3ea"
      },
      "source": [
        "## 5. Benchmark Runner\n",
        "\n",
        "Run these 3 standardized queries. Produce JSON then prose answers with citations. These are the standardized queries.\n",
        "\n",
        "*   Gross Margin Trend (or NIM if Bank)\n",
        "    *   Query: \"Report the Gross Margin (or Net Interest Margin, if a bank) over the last 5 quarters, with values.\"\n",
        "    *   Expected Output: A quarterly table of Gross Margin % (or NIM % if bank).\n",
        "\n",
        "*   Operating Expenses (Opex) YoY for 3 Years\n",
        "    *   Query: \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\"\n",
        "    *   Expected Output: A 3-year Opex table (absolute numbers and % change).\n",
        "\n",
        "*   Operating Efficiency Ratio\n",
        "    *   Query: \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\"\n",
        "    *   Expected Output: Table with Opex, Operating Income, and calculated ratio for 3 years."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bddc40",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T09:11:32.887486200Z",
          "start_time": "2025-09-30T09:09:32.019818900Z"
        },
        "id": "e7bddc40"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement benchmark runner\n",
        "# =============================\n",
        "# Part 5. Benchmark Runner\n",
        "# =============================\n",
        "\n",
        "# Define the benchmark queries\n",
        "benchmark_queries = [\n",
        "    {\n",
        "        \"name\": \"NIM Quarterly Trend\",\n",
        "        \"query\": \"Report the Net Interest Margin over the last 5 quarters, with values.\",\n",
        "        \"expected\": \"A quarterly table of NIM %\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Opex YoY 3-Year\",\n",
        "        \"query\": \"Show Operating Expenses for the last 3 fiscal years, year-on-year comparison.\",\n",
        "        \"expected\": \"A 3-year Opex table (absolute numbers and % change)\"\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"Operating Efficiency Ratio\",\n",
        "        \"query\": \"Calculate the Operating Efficiency Ratio (Opex Ã· Operating Income) for the last 3 fiscal years, showing the working.\",\n",
        "        \"expected\": \"Table with Opex, Operating Income, and calculated ratio for 3 years\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Benchmark runner\n",
        "def benchmark_pipeline(query: str, top_k: int = 12):\n",
        "    \"\"\"Pipeline for testing queries without instrumentation\"\"\"\n",
        "    \n",
        "    # Retrieve relevant docs\n",
        "    retrieved_docs = cfo_rag.hybrid_retrieve(query, top_k=top_k)\n",
        "    if not retrieved_docs:\n",
        "        return {\"error\": \"No documents retrieved.\"}\n",
        "    \n",
        "    context = \"\\n\\n\".join([f\"{doc['citation']}: {doc['text']}\" for doc in retrieved_docs])\n",
        "    \n",
        "    # Single enhanced prompt for table generation\n",
        "    table_prompt = f\"\"\"\n",
        "You are a financial analyst. Answer this query using ONLY the provided reports.\n",
        "\n",
        "Query: {query}\n",
        "\n",
        "FORMAT REQUIREMENT: Present data in the requested table format using markdown.\n",
        "\n",
        "Reports:\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "1. Extract the exact financial figures from the reports\n",
        "2. Present them in the requested table format using markdown\n",
        "3. Include citations (filename, page number) after each table\n",
        "4. If calculations are needed, show the working clearly\n",
        "5. Use proper financial notation (S$ millions, percentages, etc.)\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    \n",
        "    response = llm_model.generate_content(table_prompt)\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"citations\": [doc[\"citation\"] for doc in retrieved_docs],\n",
        "        \"answer\": response.text.strip()\n",
        "    }\n",
        "\n",
        "# Test the benchmark queries\n",
        "print(\"=== Testing Benchmark Queries (Simple) ===\")\n",
        "for q in benchmark_queries:\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"QUERY: {q['name']}\")\n",
        "    print(f\"{'='*50}\")\n",
        "    result = benchmark_pipeline(q[\"query\"])\n",
        "    print(f\"Answer:\\n{result['answer']}\")\n",
        "    print(f\"\\nCitations: {result['citations']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64dbe015",
      "metadata": {
        "id": "64dbe015"
      },
      "outputs": [],
      "source": [
        "# # Define instrumented pipeline and benchmark runner functions\n",
        "# import pandas as pd\n",
        "# import time\n",
        "# from tabulate import tabulate\n",
        "\n",
        "# # Make sure logs DataFrame is accessible (if not defined globally elsewhere)\n",
        "# try:\n",
        "#     logs\n",
        "# except NameError:\n",
        "#     logs = pd.DataFrame(columns=[\n",
        "#         'Query',\n",
        "#         'T_ingest (sec)', 'T_retrieve (sec)', 'T_rerank (ms)', 'T_reason (sec)',\n",
        "#         'T_generate (sec)', 'T_total (sec)',\n",
        "#         'Tokens', 'CacheHits', 'Tools'\n",
        "#     ])\n",
        "\n",
        "# benchmark_queries = [\n",
        "#     {\n",
        "#         \"name\": \"NIM Trend\",\n",
        "#         \"query\": \"Net Interest Margin (NIM) trend over last 5 quarters, values and 1â€“2 lines of explanation.\",\n",
        "#         \"expected\": \"Quarterly financial highlights\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"name\": \"Opex YoY\",\n",
        "#         \"query\": \"Operating Expenses (Opex) YoY for last 3 years; top 3 drivers from MD&A.\",\n",
        "#         \"expected\": \"Opex table + MD&A commentary\"\n",
        "#     },\n",
        "#     {\n",
        "#         \"name\": \"Cost-to-Income Ratio\",\n",
        "#         \"query\": \"Cost-to-Income Ratio (CTI) for last 3 years; show working + implications.\",\n",
        "#         \"expected\": \"Operating Income & Opex lines\"\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# def instrumented_pipeline(query: str, top_k: int = 5):\n",
        "#     \"\"\"\n",
        "#     Runs pipeline with instrumentation (timing + usage logging).\n",
        "#     \"\"\"\n",
        "#     global logs\n",
        "#     timings = {}\n",
        "#     start_total = time.time()\n",
        "\n",
        "#     # --- Retrieval ---\n",
        "#     retrieved_docs = cfo_rag.hybrid_retrieve(query, top_k=top_k)\n",
        "#     timings['T_retrieve'] = cfo_rag.metrics.get('T_retrieve', 0)\n",
        "#     timings['T_rerank']  = cfo_rag.metrics.get('T_rerank', 0)  # already in ms\n",
        "\n",
        "\n",
        "#     if not retrieved_docs:\n",
        "#         return {\"error\": \"No documents retrieved.\"}\n",
        "\n",
        "#     # --- Build context ---\n",
        "#     context = \"\\n\\n\".join([f\"{doc['citation']}: {doc['text']}\" for doc in retrieved_docs])\n",
        "\n",
        "#     # --- Reasoning step ---\n",
        "#     start_reason = time.time()\n",
        "#     reasoning_prompt = f\"\"\"\n",
        "#     Summarize the key financial figures and trends in a structured way.\n",
        "#     Do not generate the final answer yet.\n",
        "\n",
        "#     Query: {query}\n",
        "#     Reports:\n",
        "#     {context}\n",
        "#     \"\"\"\n",
        "#     reasoning_output = llm_model.generate_content(reasoning_prompt)\n",
        "#     timings['T_reason'] = time.time() - start_reason\n",
        "\n",
        "#     # --- Generation step ---\n",
        "#     start_generate = time.time()\n",
        "#     final_prompt = f\"\"\"\n",
        "#     You are a financial analyst assistant.\n",
        "#     Based on the reasoning and reports, provide the final answer with citations.\n",
        "\n",
        "#     Reasoning:\n",
        "#     {reasoning_output.text}\n",
        "\n",
        "#     Query: {query}\n",
        "#     \"\"\"\n",
        "#     response = llm_model.generate_content(final_prompt)\n",
        "#     timings['T_generate'] = time.time() - start_generate\n",
        "\n",
        "#     # --- Total ---\n",
        "#     timings['T_total'] = time.time() - start_total\n",
        "\n",
        "#     # --- Log row ---\n",
        "#     new_row = {\n",
        "#         'Query': query,\n",
        "#         'T_ingest (sec)': cfo_rag.metrics.get('T_ingest', 0),\n",
        "#         'T_retrieve (sec)': timings['T_retrieve'],\n",
        "#         'T_rerank (ms)': timings['T_rerank'],\n",
        "#         'T_reason (sec)': timings['T_reason'],\n",
        "#         'T_generate (sec)': timings['T_generate'],\n",
        "#         'T_total (sec)': timings['T_total'],\n",
        "#         'Tokens': None, # Placeholder\n",
        "#         'CacheHits': 0, # Placeholder\n",
        "#         'Tools': ['Retriever', 'Reranker', 'LLM'] # Placeholder\n",
        "#     }\n",
        "#     logs = pd.concat([logs, pd.DataFrame([new_row])], ignore_index=True)\n",
        "\n",
        "#     # --- Print per-query summary ---\n",
        "#     summary = [\n",
        "#         [\"Retrieve\", f\"{timings['T_retrieve']:.4f} sec\"],\n",
        "#         [\"Rerank\",   f\"{timings['T_rerank']:.3f} ms\"],\n",
        "#         [\"Reason\",   f\"{timings['T_reason']:.4f} sec\"],\n",
        "#         [\"Generate\", f\"{timings['T_generate']:.4f} sec\"],\n",
        "#         [\"Total\",    f\"{timings['T_total']:.4f} sec\"],\n",
        "#     ]\n",
        "#     print(f\"\\n=== Timing Summary for Query: {query} ===\")\n",
        "#     print(tabulate(summary, headers=[\"Stage\", \"Time\"], tablefmt=\"github\"))\n",
        "\n",
        "\n",
        "#     return {\n",
        "#         \"query\": query,\n",
        "#         \"citations\": [doc[\"citation\"] for doc in retrieved_docs],\n",
        "#         \"reasoning\": reasoning_output.text.strip(),\n",
        "#         \"answer\": response.text.strip()\n",
        "#     }\n",
        "\n",
        "\n",
        "# def run_benchmark_instrumented(queries, top_k=5):\n",
        "#     results = []\n",
        "#     for q in queries:\n",
        "#         print(f\"\\n=== Running Instrumented Benchmark: {q['name']} ===\")\n",
        "#         output = instrumented_pipeline(q[\"query\"], top_k=top_k)\n",
        "#         results.append({\n",
        "#             \"name\": q[\"name\"],\n",
        "#             \"query\": q[\"query\"],\n",
        "#             \"expected\": q[\"expected\"],\n",
        "#             \"citations\": output.get(\"citations\", []),\n",
        "#             \"reasoning\": output.get(\"reasoning\", \"N/A\"),\n",
        "#             \"answer\": output.get(\"answer\", \"Error: no answer\")\n",
        "#         })\n",
        "#     return results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683ebeda",
      "metadata": {
        "id": "683ebeda"
      },
      "source": [
        "## 6. Instrumentation\n",
        "\n",
        "Log timings: T_ingest, T_retrieve, T_rerank, T_reason, T_generate, T_total. Log tokens, cache hits, tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5425de5",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T09:13:04.577246400Z",
          "start_time": "2025-09-30T09:11:32.887486200Z"
        },
        "id": "d5425de5"
      },
      "outputs": [],
      "source": [
        "# =============================\n",
        "# Part 6. Instrumentation (Final Version)\n",
        "# =============================\n",
        "\n",
        "import pandas as pd\n",
        "import time\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Initialise performance logging DataFrame\n",
        "logs = pd.DataFrame(columns=[\n",
        "    'Query Name', 'T_retrieve (sec)', 'T_rerank (ms)', \n",
        "    'T_generate (sec)', 'T_total (sec)', 'Tokens', 'CacheHits', 'Tools'\n",
        "])\n",
        "\n",
        "def instrumented_pipeline(query: str, query_name: str, top_k: int = 5):\n",
        "    \"\"\"Enhanced pipeline with complete performance instrumentation\"\"\"\n",
        "    \n",
        "    global logs\n",
        "    timings = {}\n",
        "    start_total = time.perf_counter()\n",
        "    \n",
        "    # --- Retrieval & Reranking ---\n",
        "    retrieved_docs = cfo_rag.hybrid_retrieve(query, top_k=top_k)\n",
        "    timings['T_retrieve'] = cfo_rag.metrics.get('T_retrieve', 0)\n",
        "    timings['T_rerank'] = cfo_rag.metrics.get('T_rerank', 0)\n",
        "    \n",
        "    if not retrieved_docs:\n",
        "        return {\"error\": \"No documents retrieved.\"}\n",
        "    \n",
        "    context = \"\\n\".join([f\"Source {doc['citation']}: {doc['text']}\" for doc in retrieved_docs])\n",
        "    \n",
        "    # --- Dynamic Prompt Formatting ---\n",
        "    if \"quarter\" in query.lower():\n",
        "        format_instruction = \"Present data in a quarterly markdown table with columns for Quarter and the metric.\"\n",
        "    elif \"year-on-year\" in query.lower():\n",
        "        format_instruction = \"Present data in a YoY comparison markdown table with columns for Year, Value, and % Change.\"\n",
        "    elif \"ratio\" in query.lower() and \"working\" in query.lower():\n",
        "        format_instruction = \"Show the calculation working in a markdown table with columns for each component and the final Ratio.\"\n",
        "    else:\n",
        "        format_instruction = \"Present data in a clear markdown table.\"\n",
        "    \n",
        "    # --- Generation ---\n",
        "    table_prompt = f\"\"\"You are an expert financial analyst. Answer the query using ONLY the provided reports.\n",
        "\n",
        "Query: {query}\n",
        "FORMAT REQUIREMENT: {format_instruction}\n",
        "\n",
        "Reports:\n",
        "{context}\n",
        "\n",
        "Instructions:\n",
        "1. Extract the exact financial figures from the reports.\n",
        "2. Present them in the requested markdown table format.\n",
        "3. After the table, list all source citations used.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "    \n",
        "    start_generate = time.perf_counter()\n",
        "    response = llm_model.generate_content(table_prompt)\n",
        "    timings['T_generate'] = time.perf_counter() - start_generate\n",
        "    timings['T_total'] = time.perf_counter() - start_total\n",
        "    \n",
        "    # --- Logging ---\n",
        "    try:\n",
        "        token_count = response.usage_metadata.total_token_count if hasattr(response, 'usage_metadata') else 0\n",
        "    except (AttributeError, TypeError):\n",
        "        token_count = 0\n",
        "    \n",
        "    new_log = {\n",
        "        'Query Name': query_name,\n",
        "        'T_retrieve (sec)': timings['T_retrieve'],\n",
        "        'T_rerank (ms)': timings['T_rerank'],\n",
        "        'T_generate (sec)': timings['T_generate'],\n",
        "        'T_total (sec)': timings['T_total'],\n",
        "        'Tokens': token_count,\n",
        "        'CacheHits': 0,\n",
        "        'Tools': 'LLM-formatter'\n",
        "    }\n",
        "    \n",
        "    logs = pd.concat([logs, pd.DataFrame([new_log])], ignore_index=True)\n",
        "    \n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"citations\": [doc[\"citation\"] for doc in retrieved_docs],\n",
        "        \"answer\": response.text.strip()\n",
        "    }\n",
        "\n",
        "def run_instrumented_benchmarks(queries, top_k=5):\n",
        "    \"\"\"Runs the instrumented pipeline for all benchmark queries.\"\"\"\n",
        "    all_results = []\n",
        "    for q in queries:\n",
        "        print(f\"--- Running Instrumented Benchmark: {q['name']} ---\")\n",
        "        output = instrumented_pipeline(q[\"query\"], q[\"name\"], top_k=top_k)\n",
        "        all_results.append({\"name\": q[\"name\"], **output})\n",
        "    return all_results\n",
        "\n",
        "# Execute instrumented benchmarks\n",
        "print(\"=== Starting Instrumented Benchmark Run ===\")\n",
        "benchmark_results = run_instrumented_benchmarks(benchmark_queries, top_k=7)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"INSTRUMENTED BENCHMARK ANSWERS\")\n",
        "print(\"=\"*50)\n",
        "for res in benchmark_results:\n",
        "    print(f\"\\n--- Query: {res['name']} ---\")\n",
        "    print(f\"Answer:\\n{res['answer']}\")\n",
        "    print(f\"\\nCitations: {res['citations']}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"PERFORMANCE LOGS\")\n",
        "print(\"=\"*50)\n",
        "display(logs)\n",
        "\n",
        "# Performance Summary\n",
        "if not logs.empty:\n",
        "    print(f\"\\nðŸ“Š PERFORMANCE SUMMARY:\")\n",
        "    print(f\"Average Response Time: {logs['T_total (sec)'].mean():.3f}s\")\n",
        "    print(f\"P95 Response Time: {logs['T_total (sec)'].quantile(0.95):.3f}s\")\n",
        "    print(f\"Average Tokens Used: {logs['Tokens'].mean():.0f}\")\n",
        "    print(f\"Fastest Query: {logs.loc[logs['T_total (sec)'].idxmin(), 'Query Name']} ({logs['T_total (sec)'].min():.3f}s)\")\n",
        "    print(f\"Slowest Query: {logs.loc[logs['T_total (sec)'].idxmax(), 'Query Name']} ({logs['T_total (sec)'].max():.3f}s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "722d86ce",
      "metadata": {
        "id": "722d86ce"
      },
      "source": [
        "### Running Baseline Pipeline (Part 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67d3e343",
      "metadata": {
        "id": "67d3e343"
      },
      "outputs": [],
      "source": [
        "# # Run the baseline pipeline example\n",
        "# result = baseline_pipeline(\"Net Interest Margin trend over the past 3 years\", top_k=3)\n",
        "# print(\"=== Baseline Answer ===\")\n",
        "# print(result[\"answer\"])\n",
        "# print(\"\\nCitations:\", result[\"citations\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a643fe34",
      "metadata": {
        "id": "a643fe34"
      },
      "source": [
        "### Running Instrumented Benchmark (Part 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3163da73",
      "metadata": {
        "id": "3163da73"
      },
      "outputs": [],
      "source": [
        "# # Run the instrumented benchmarks\n",
        "# benchmark_results_instrumented = run_benchmark_instrumented(benchmark_queries, top_k=5)\n",
        "\n",
        "# # Show instrumentation logs (DataFrame)\n",
        "# display(logs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8c01bf4",
      "metadata": {
        "id": "e8c01bf4"
      },
      "source": [
        "## 7. Optimizations\n",
        "\n",
        "**Required Optimizations**\n",
        "\n",
        "Each team must implement at least:\n",
        "*   2 retrieval optimizations (e.g., hybrid BM25+vector, smaller embeddings, dynamic k).\n",
        "*   1 caching optimization (query cache or ratio cache).\n",
        "*   1 agentic optimization (plan pruning, parallel sub-queries).\n",
        "*   1 system optimization (async I/O, batch embedding, memory-mapped vectors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "783f0e2e",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T08:58:38.646714Z",
          "start_time": "2025-09-30T08:58:38.639026800Z"
        },
        "id": "783f0e2e"
      },
      "outputs": [],
      "source": [
        "# TODO: Implement optimizations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a91ce833",
      "metadata": {
        "id": "a91ce833"
      },
      "source": [
        "## 8. Results & Plots\n",
        "\n",
        "Show baseline vs optimized. Include latency plots (p50/p95) and accuracy tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d96550f3",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-09-30T08:58:38.648719100Z",
          "start_time": "2025-09-30T08:58:38.642546300Z"
        },
        "id": "d96550f3"
      },
      "outputs": [],
      "source": [
        "# TODO: Generate plots with matplotlib\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "s2am56tlpjY9",
      "metadata": {
        "id": "s2am56tlpjY9"
      },
      "source": [
        "testing"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
